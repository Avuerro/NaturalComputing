{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Computing - Assignment 2 - Swarm Intelligence\n",
    "## Exercise 3 - PSO\n",
    "#### Submission by group 25 (Chihab Amghane, Max Driessen, Jordy Naus)\n",
    "\n",
    "This file contains our solution to exercise 3 of the \"Swarm Intelligence\" assignment of the Natural Computing course.\n",
    "\n",
    "**NOTE:** (\"!!!\" denotes remarks/TODOs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans # !!! Temporary (?), see K-means clustering algorithm below\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PSO clustering algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PSO_clustering(n_clusters, n_particles, n_iterations, data, omega=0.7298, alpha_1=1.49618, alpha_2=1.49618, verbose=False):\n",
    "    # Compute number of dimensions\n",
    "    n_dims = data.shape[1]\n",
    "    \n",
    "    # Compute range of input data in every dimension (to ensure reasonable initial particles)\n",
    "    ranges = [(min(data[:,dim]), max(data[:,dim])) for dim in range(n_dims)]\n",
    "    \n",
    "    # Initialize particles with random centroids\n",
    "    particles, speeds = [], []\n",
    "    for n in range(n_particles):\n",
    "        # Generate a random particle by creating the desired number of random centroids\n",
    "        particle = np.array([[random.uniform(r[0]-10,r[1]+10) for r in ranges] for c in range(n_clusters)])\n",
    "        # Add the particle to the list of particles\n",
    "        particles.append(particle)\n",
    "        # Add a speed of 0 to the list of speeds !!! 0 or random?\n",
    "        speeds.append(np.zeros(particle.shape)) # speeds.append(np.random.uniform(-1,1,particle.shape))\n",
    "    \n",
    "    # Initialize local & global bests\n",
    "    local_bests = [([],float('inf')) for particle in particles]\n",
    "    global_best = ([],float('inf'))\n",
    "    \n",
    "    # Perform iterations until termination\n",
    "    for iteration in range(n_iterations):\n",
    "        if verbose:\n",
    "            print(f\"iteration {iteration}\")\n",
    "        \n",
    "        # For each particle:\n",
    "        for i,particle in enumerate(particles):\n",
    "            # Compute fitness (quantization error)\n",
    "            fitness = quantization_error(particle, data)\n",
    "            # Update local best\n",
    "            if fitness < local_bests[i][1]:\n",
    "                local_bests[i] = (particle, fitness)\n",
    "        \n",
    "        # Update global best\n",
    "        best_in_iteration = np.argmin([local_best[1] for local_best in local_bests])\n",
    "        if local_bests[best_in_iteration][1] < global_best[1]:\n",
    "            global_best = local_bests[best_in_iteration]\n",
    "        \n",
    "        # Update particles\n",
    "        for i in range(n_particles):\n",
    "            r_1 = np.random.uniform(0,1,particle.shape)\n",
    "            r_2 = np.random.uniform(0,1,particle.shape)\n",
    "            speeds[i] = omega*speeds[i] \\\n",
    "                      + np.multiply(alpha_1*r_1, local_bests[i][0]-particles[i]) \\\n",
    "                      + np.multiply(alpha_2*r_2, global_best[0]-particles[i])\n",
    "            particles[i] = particles[i] + speeds[i]\n",
    "    \n",
    "    # Return the global best\n",
    "    return global_best[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantization_error(particle, X):\n",
    "    # Retrieve number of clusters\n",
    "    n_clusters = len(particle)\n",
    "    \n",
    "    # Assign datapoints to clusters; store computed distances to corresponding centroids\n",
    "    clusters = {i:[] for i in range(n_clusters)}\n",
    "    for datapoint in X:\n",
    "        # Compute Euler distance between this datapoint and all centroids\n",
    "        distances = [np.sqrt(np.sum((centroid-datapoint)**2)) for centroid in particle]\n",
    "        # Assign datapoint to centroid with smallest distance, by storing distance in the corresponding array\n",
    "        clusters[np.argmin(distances)].append(min(distances))\n",
    "    \n",
    "    # Compute quantization error as described in paper\n",
    "    error = 0\n",
    "    for c in clusters:\n",
    "        error += sum(clusters[c])/len(clusters[c]) if clusters[c] else 0\n",
    "    error = error/n_clusters\n",
    "    \n",
    "    # Return the computed error\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clustering algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! We should probably implement this ourselves...\n",
    "def KMeans_clustering(n_clusters, n_init, max_iter, data):\n",
    "    kmeans = KMeans(init=\"random\", n_clusters=n_clusters, n_init=n_init, max_iter=max_iter)\n",
    "    kmeans.fit(data)\n",
    "    return kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading datasets\n",
    "\n",
    "#### Artificial dataset I \n",
    "(as described [here](https://scholar.google.nl/scholar?hl=nl&as_sdt=0%2C5&q=Van+der+Merwe%2C+D.+W.%2C+and+Andries+Petrus+Engelbrecht.+%22Data+clustering+using+particle+swarm+optimization%22&btnG=))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "artificial_dataset_size = 400\n",
    "\n",
    "artificial_X = np.array([(random.uniform(-1,1), random.uniform(-1,1)) for i in range(artificial_dataset_size)])\n",
    "artificial_Y = np.array([int((z_1 >= 0.7) or ((z_1 <= 0.3) and (z_2 >= -0.2 - z_1))) for (z_1,z_2) in artificial_X])\n",
    "artificial_n_clusters = len(np.unique(artificial_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "iris_X = iris.data\n",
    "iris_Y = iris.target\n",
    "iris_n_clusters = len(np.unique(iris_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "!!! Don't know if this is sufficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Artificial dataset I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "artificial_solution_PSO = PSO_clustering(artificial_n_clusters, 5, 100, artificial_X)\n",
    "artificial_solution_KMeans = KMeans_clustering(artificial_n_clusters, 5, 100, artificial_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSO quantization error on artificial dataset I: 0.37508839492160023\n",
      "KMeans quantization error on artificial dataset I: 0.5676370503289512\n"
     ]
    }
   ],
   "source": [
    "artificial_error_PSO = quantization_error(artificial_solution_PSO, artificial_X)\n",
    "artificial_error_KMeans = quantization_error(artificial_solution_KMeans, artificial_X)\n",
    "print(f\"PSO quantization error on artificial dataset I: {artificial_error_PSO}\")\n",
    "print(f\"KMeans quantization error on artificial dataset I: {artificial_error_KMeans}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_solution_PSO = PSO_clustering(iris_n_clusters, 5, 100, iris_X)\n",
    "iris_solution_KMeans = KMeans_clustering(iris_n_clusters, 5, 100, iris_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSO quantization error on iris dataset: 0.6295904619016034\n",
      "KMeans quantization error on iris dataset: 0.6465653848597094\n"
     ]
    }
   ],
   "source": [
    "iris_error_PSO = quantization_error(iris_solution_PSO, iris_X)\n",
    "iris_error_KMeans = quantization_error(iris_solution_KMeans, iris_X)\n",
    "print(f\"PSO quantization error on iris dataset: {iris_error_PSO}\")\n",
    "print(f\"KMeans quantization error on iris dataset: {iris_error_KMeans}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
