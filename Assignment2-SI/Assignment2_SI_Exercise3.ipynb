{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Computing - Assignment 2 - Swarm Intelligence\n",
    "## Exercise 3 - PSO\n",
    "#### Submission by group 25 (Chihab Amghane, Max Driessen, Jordy Naus)\n",
    "\n",
    "This file contains our solution to exercise 3 of the \"Swarm Intelligence\" assignment of the Natural Computing course.\n",
    "\n",
    "**NOTE:** (\"!!!\" denotes remarks/TODOs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans # !!! Temporary (?), see K-means clustering algorithm below\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import random\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PSO clustering algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PSO_clustering(n_clusters, n_particles, n_iterations, data, omega=0.7298, alpha_1=1.49618, alpha_2=1.49618, verbose=False):\n",
    "    # Compute number of dimensions\n",
    "    n_dims = data.shape[1]\n",
    "    \n",
    "    # Compute range of input data in every dimension (to ensure reasonable initial particles)\n",
    "    ranges = [(min(data[:,dim]), max(data[:,dim])) for dim in range(n_dims)]\n",
    "    \n",
    "    # Initialize particles with random centroids\n",
    "    particles, speeds = [], []\n",
    "    for n in range(n_particles):\n",
    "        # Generate a random particle by creating the desired number of random centroids\n",
    "        particle = np.array([[random.uniform(r[0]-10,r[1]+10) for r in ranges] for c in range(n_clusters)])\n",
    "        # Add the particle to the list of particles\n",
    "        particles.append(particle)\n",
    "        # Add a speed of 0 to the list of speeds !!! 0 or random?\n",
    "        speeds.append(np.zeros(particle.shape)) # speeds.append(np.random.uniform(-1,1,particle.shape))\n",
    "    \n",
    "    # Initialize local & global bests\n",
    "    local_bests = [([],float('inf')) for particle in particles]\n",
    "    global_best = ([],float('inf'))\n",
    "    \n",
    "    # Perform iterations until termination\n",
    "    for iteration in range(n_iterations):\n",
    "        if verbose:\n",
    "            print(f\"iteration {iteration}\")\n",
    "        \n",
    "        # For each particle:\n",
    "        for i,particle in enumerate(particles):\n",
    "            # Compute fitness (quantization error)\n",
    "            fitness = quantization_error(particle, data)\n",
    "            # Update local best\n",
    "            if fitness < local_bests[i][1]:\n",
    "                local_bests[i] = (particle, fitness)\n",
    "        \n",
    "        # Update global best\n",
    "        best_in_iteration = np.argmin([local_best[1] for local_best in local_bests])\n",
    "        if local_bests[best_in_iteration][1] < global_best[1]:\n",
    "            global_best = local_bests[best_in_iteration]\n",
    "        \n",
    "        # Update particles\n",
    "        for i in range(n_particles):\n",
    "            r_1 = np.random.uniform(0,1,particle.shape)\n",
    "            r_2 = np.random.uniform(0,1,particle.shape)\n",
    "            speeds[i] = omega*speeds[i] \\\n",
    "                      + np.multiply(alpha_1*r_1, local_bests[i][0]-particles[i]) \\\n",
    "                      + np.multiply(alpha_2*r_2, global_best[0]-particles[i])\n",
    "            particles[i] = particles[i] + speeds[i]\n",
    "    \n",
    "    # Return the global best\n",
    "    return global_best[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantization_error(particle, X):\n",
    "    # Retrieve number of clusters\n",
    "    n_clusters = len(particle)\n",
    "    \n",
    "    # Assign datapoints to clusters; store computed distances to corresponding centroids\n",
    "    clusters = {i:[] for i in range(n_clusters)}\n",
    "    for datapoint in X:\n",
    "        # Compute Euler distance between this datapoint and all centroids\n",
    "        distances = [np.sqrt(np.sum((centroid-datapoint)**2)) for centroid in particle]\n",
    "        # Assign datapoint to centroid with smallest distance, by storing distance in the corresponding array\n",
    "        clusters[np.argmin(distances)].append(min(distances))\n",
    "    \n",
    "    # Compute quantization error as described in paper\n",
    "    error = 0\n",
    "    for c in clusters:\n",
    "        error += sum(clusters[c])/len(clusters[c]) if clusters[c] else 0\n",
    "    error = error/n_clusters\n",
    "    \n",
    "    # Return the computed error\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clustering algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_closest_centroid(data_vector, centroids):\n",
    "    closest_centroid = None # initially no closest centroid..\n",
    "    min_distance = np.inf \n",
    "    for index, centroid in enumerate(centroids):\n",
    "        distance = np.sqrt(np.sum( (data_vector - centroid) ** 2 ))\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_centroid = index \n",
    "    return closest_centroid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom implementation\n",
    "\"\"\"\n",
    "\n",
    "SHOULD THE CLUSTERS BE RANDOMLY INITIALIZED IN THE BEGINNING?\n",
    "\n",
    "\"\"\"\n",
    "def KMeans_clustering(n_clusters, n_init, max_iter, data):\n",
    "    \n",
    "    input_dimension = data.shape[1]\n",
    "    number_of_data_vectors = data.shape[0]\n",
    "\n",
    "    ranges = [(min(data[:,dim]), max(data[:,dim])) for dim in range(input_dimension)]\n",
    "\n",
    "    cluster_centroids =  np.array([[random.uniform(r[0],r[1]) for r in ranges] for c in range(n_clusters)])\n",
    "    clusters = [[] for x in range(n_clusters)]\n",
    "     \n",
    "    ## repeat for n_init iterations\n",
    "    for iteration in range(max_iter):\n",
    "        \n",
    "        # calculate distance for each data vector\n",
    "        # determine closest centroid vector\n",
    "\n",
    "        for data_vector in data:\n",
    "            closest_centroid = determine_closest_centroid(data_vector,cluster_centroids)\n",
    "            clusters[closest_centroid].append(data_vector)\n",
    "            \n",
    "        # recalculate cluster centroid vectors\n",
    "        for index, centroid in enumerate(cluster_centroids):\n",
    "            if (len(clusters[index]) == 0): # clusters could be empty..\n",
    "                centroid = centroid\n",
    "            else:\n",
    "                centroid = (1 / len(clusters[index])) * np.sum(clusters[index])\n",
    "        \n",
    "    return cluster_centroids\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! We should probably implement this ourselves...\n",
    "# def KMeans_clustering(n_clusters, n_init, max_iter, data):\n",
    "#     kmeans = KMeans(init=\"random\", n_clusters=n_clusters, n_init=n_init, max_iter=max_iter)\n",
    "#     kmeans.fit(data)\n",
    "#     return kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading datasets\n",
    "\n",
    "#### Artificial dataset I \n",
    "(as described [here](https://scholar.google.nl/scholar?hl=nl&as_sdt=0%2C5&q=Van+der+Merwe%2C+D.+W.%2C+and+Andries+Petrus+Engelbrecht.+%22Data+clustering+using+particle+swarm+optimization%22&btnG=))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "artificial_dataset_size = 400\n",
    "\n",
    "artificial_X = np.array([(random.uniform(-1,1), random.uniform(-1,1)) for i in range(artificial_dataset_size)])\n",
    "artificial_Y = np.array([int((z_1 >= 0.7) or ((z_1 <= 0.3) and (z_2 >= -0.2 - z_1))) for (z_1,z_2) in artificial_X])\n",
    "artificial_n_clusters = len(np.unique(artificial_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "iris_X = iris.data\n",
    "iris_Y = iris.target\n",
    "iris_n_clusters = len(np.unique(iris_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "!!! Don't know if this is sufficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Artificial dataset I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "artificial_solution_PSO = PSO_clustering(artificial_n_clusters, 5, 100, artificial_X)\n",
    "artificial_solution_KMeans = KMeans_clustering(artificial_n_clusters, 5, 100, artificial_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSO quantization error on artificial dataset I: 0.38875105745782745\n",
      "KMeans quantization error on artificial dataset I: 0.6480355898644439\n"
     ]
    }
   ],
   "source": [
    "artificial_error_PSO = quantization_error(artificial_solution_PSO, artificial_X)\n",
    "artificial_error_KMeans = quantization_error(artificial_solution_KMeans, artificial_X)\n",
    "print(f\"PSO quantization error on artificial dataset I: {artificial_error_PSO}\")\n",
    "print(f\"KMeans quantization error on artificial dataset I: {artificial_error_KMeans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 30 iterations\n",
    "pso_quantization_errors = []\n",
    "kmeans_quantization_errors = []\n",
    "\n",
    "for trial in range(30):\n",
    "    artificial_solution_PSO = PSO_clustering(artificial_n_clusters, 5, 100, artificial_X)\n",
    "    artificial_solution_KMeans = KMeans_clustering(artificial_n_clusters, 5, 100, artificial_X)\n",
    "    artificial_error_PSO = quantization_error(artificial_solution_PSO, artificial_X)\n",
    "    artificial_error_KMeans = quantization_error(artificial_solution_KMeans, artificial_X)\n",
    "    pso_quantization_errors.append(artificial_error_PSO)\n",
    "    kmeans_quantization_errors.append(artificial_error_KMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average kmeans quantization error over 30 simulations 0.7220069497733106 ± 0.08834767444758052\n",
      "Average PSO quantization error over 30 simulations 0.38875178903746893 ± 3.659986300083889e-06\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average kmeans quantization error over 30 simulations {np.average(kmeans_quantization_errors)} ± {np.std(kmeans_quantization_errors)}\")\n",
    "print(f\"Average PSO quantization error over 30 simulations {np.average(pso_quantization_errors)} ± {np.std(pso_quantization_errors)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_solution_PSO = PSO_clustering(iris_n_clusters, 5, 100, iris_X)\n",
    "iris_solution_KMeans = KMeans_clustering(iris_n_clusters, 5, 100, iris_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSO quantization error on iris dataset: 0.6298520657504828\n",
      "KMeans quantization error on iris dataset: 2.2650510601209444\n"
     ]
    }
   ],
   "source": [
    "iris_error_PSO = quantization_error(iris_solution_PSO, iris_X)\n",
    "iris_error_KMeans = quantization_error(iris_solution_KMeans, iris_X)\n",
    "print(f\"PSO quantization error on iris dataset: {iris_error_PSO}\")\n",
    "print(f\"KMeans quantization error on iris dataset: {iris_error_KMeans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
