{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Computing - Project\n",
    "#### Submission by group 25 (Chihab Amghane, Max Driessen, Jordy Naus)\n",
    "\n",
    "The code below uses the [DEAP framework](https://github.com/deap/deap), which is an intuitive framework for evolutionary algorithms and genetic programming. We adapted several components of this framework to match more closely with the [WANN implementation](https://github.com/google/brain-tokyo-workshop/tree/master/WANNRelease)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEAP\n",
    "from deap import gp, base, tools, creator, algorithms\n",
    "\n",
    "# Data processing and plotting\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Requirements for the algorithm\n",
    "from operator import attrgetter\n",
    "from functools import partial\n",
    "\n",
    "# Standard python imports\n",
    "import random, pickle, math, re, os\n",
    "import numpy as np\n",
    "\n",
    "# Magic for inline plots\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp(x):\n",
    "    return np.exp(np.clip(x, -float('inf'), 709.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "DATASET = \"MNIST\" # choose from {\"MNIST\", \"Fashion-MNIST\"} \n",
    "N_CLASSES_TO_USE = 10 # at most 10\n",
    "\n",
    "# Individual trees\n",
    "P_INITIAL_CONNECTION = 0.05\n",
    "\n",
    "# Fitness\n",
    "N_SAMPLES_TO_TEST = 200\n",
    "WEIGHTS_TO_TEST = [-2, -1, 1, 2]\n",
    "\n",
    "# Parent selection\n",
    "TOURNAMENT_SIZE = 32\n",
    "\n",
    "# Mutation (probabilities should sum to 1)\n",
    "P_MUTATE_ACTIVATION = 0.5\n",
    "P_ADD_NODE = 0.25\n",
    "P_ADD_CONNECTION = 0.2\n",
    "P_ENABLE_CONNECTION = 0.05\n",
    "\n",
    "# Evolution\n",
    "POPULATION_SIZE = 250\n",
    "N_GENERATIONS = 1000\n",
    "CULL_RATIO = 0.2\n",
    "ELITE_RATIO = 0.2\n",
    "\n",
    "\n",
    "# Filenames\n",
    "RESULTS_FILENAME = \"results.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the correct data filename\n",
    "filename = f\"{DATASET}-{N_CLASSES_TO_USE}.pkl\"\n",
    "\n",
    "# If the data has not yet been preprocessed in the specified way, do so now\n",
    "if not os.path.exists(os.path.join(\"data\", filename)):\n",
    "    print(\"Preprocessed dataset does not exist yet, creating now.\")\n",
    "    os.system(f\"python Preprocessing.py -d {DATASET} -c {N_CLASSES_TO_USE}\")\n",
    "\n",
    "# Load the preprocessed data\n",
    "with open(os.path.join(\"data\", filename), \"rb\") as f:\n",
    "    (X_train, Y_train), (X_test, Y_test) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define aggregator (weighted sum)\n",
    "def aggregate(w, args):\n",
    "    return w*sum(args)\n",
    "\n",
    "# Define operators (with a variable number of inputs)\n",
    "def linear(w, *args):\n",
    "    return aggregate(w, args)\n",
    "\n",
    "def step(w, *args):\n",
    "    return float(aggregate(w, args) > 0)\n",
    "\n",
    "def sine(w, *args):\n",
    "    return np.sin(np.pi*aggregate(w, args))\n",
    "\n",
    "def gaussian(w, *args):\n",
    "    return exp(-np.multiply(aggregate(w, args), aggregate(w, args))/2.0)\n",
    "\n",
    "def tanh(w, *args):\n",
    "    return np.tanh(aggregate(w, args))\n",
    "\n",
    "def sigmoid(w, *args):\n",
    "    return (np.tanh(aggregate(w, args)/2.0) + 1.0)/2.0\n",
    "\n",
    "def inverse(w, *args):\n",
    "    return -aggregate(w, args)\n",
    "\n",
    "def absolute(w, *args):\n",
    "    return abs(aggregate(w, args))\n",
    "\n",
    "def relu(w, *args):\n",
    "    return np.maximum(0.0, aggregate(w, args))\n",
    "\n",
    "def cosine(w, *args):\n",
    "    return np.cos(np.pi*aggregate(w, args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dictionary of functions for compiling\n",
    "function_context = {'linear':linear, 'relu':relu, 'step':step, 'sine':sine, 'gaussian':gaussian, 'tanh':tanh,  \n",
    "                    'sigmoid':sigmoid, 'inverse':inverse, 'absolute':absolute, 'cosine':cosine}\n",
    "\n",
    "# Create lists of function and argument names\n",
    "function_names = list(function_context.keys())\n",
    "argument_names = [f\"ARG{i}\" for i in range(X_train.shape[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining individuals\n",
    "\n",
    "##### Defining nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic Node class\n",
    "class Node:\n",
    "    def __init__(self, name):\n",
    "        # Each node has a name and a list of parents\n",
    "        self.name = name\n",
    "        self.parents = []\n",
    "    \n",
    "    def __str__(self):\n",
    "        raise NotImplementedError(\"String function is only implemented for subclasses\")\n",
    "\n",
    "# Class for terminal nodes (inputs)\n",
    "class TerminalNode(Node):\n",
    "    def __init__(self, name):\n",
    "        super().__init__(name)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "# Class for primitive nodes (hidden + outputs)\n",
    "class PrimitiveNode(Node):\n",
    "    def __init__(self, name):\n",
    "        # Primitive nodes also have lists of children and disabled children\n",
    "        super().__init__(name)\n",
    "        self.children = []\n",
    "        self.disabled = []\n",
    "\n",
    "    def __str__(self):\n",
    "        # Primitive nodes are formatted as \"name(child1, child2, ...)\"\n",
    "        return f\"{self.name}(w, {', '.join([str(child) for child in self.children])})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining individuals/multi-class trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for multi-output trees\n",
    "class MultiClassTree:\n",
    "    def __init__(self, n_inputs, n_outputs, p_initial_connection):\n",
    "        # Initialize lists of input, output and internal nodes\n",
    "        self.inputs = [TerminalNode(argument_names[i]) for i in range(n_inputs)]\n",
    "        self.outputs = [PrimitiveNode(\"linear\") for _ in range(n_outputs)]\n",
    "        self.hidden = []\n",
    "        self.born = -1\n",
    "        \n",
    "        # Add initial connections\n",
    "        self.n_connections = 0\n",
    "        for output in self.outputs:\n",
    "            # With a chance of P_INITIAL_CONNECTION, the connection is enabled, otherwhise it is disabled\n",
    "            for child in self.inputs:\n",
    "                if random.random() < P_INITIAL_CONNECTION:\n",
    "                    output.children.append(child)\n",
    "                else:\n",
    "                    output.disabled.append(child)\n",
    "                child.parents.append(output)\n",
    "                \n",
    "            # If an output has no enabled children, one of the children is enabled to make the tree valid\n",
    "            if not output.children:\n",
    "                child = random.choice(self.inputs)\n",
    "                output.disabled.remove(child)\n",
    "                output.children.append(child)\n",
    "            \n",
    "            # Update the number of enabled connections in the tree\n",
    "            self.n_connections += len(output.children)\n",
    "\n",
    "    def __str__(self):\n",
    "        # Printing the tree only prints the number of hidden nodes and enabled connections\n",
    "        return f\"MultiClassTree with {len(self.hidden)} hidden nodes and {self.n_connections} connections\"\\\n",
    "                + (f\", born in generation {self.born}\" if self.born >= 0 else \"\")\n",
    "    \n",
    "    def get_strings(self):\n",
    "         # (Recursively) parsing output function strings, to parse the tree for evaluation\n",
    "        try:\n",
    "            return [str(output) for output in self.outputs]\n",
    "        except RecursionError:\n",
    "            print(\"Maximum recursion depth reached\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initializing the DEAP toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize the toolbox which will contain all sorts of functions for the genetic programming process\n",
    "toolbox = base.Toolbox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classes for fitness and individuals (using DEAP's creator module)\n",
    "creator.create(\"Fitness\", base.Fitness, weights=(-1.0, -1.0, 1.0))\n",
    "creator.create(\"Individual\", MultiClassTree, fitness=creator.Fitness, rank=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how to initialize an individual or population\n",
    "toolbox.register(\"individual\", creator.Individual, X_train.shape[1], N_CLASSES_TO_USE, P_INITIAL_CONNECTION)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitness function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compiling multi-class trees into functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling a tree into a function\n",
    "def compile_multiclasstree(tree):\n",
    "    # Parse trees to strings for all outputs\n",
    "    strings = tree.get_strings()\n",
    "    \n",
    "    # Convert the string to lambda functions using eval() and the proper function context\n",
    "    funcs = [eval(f\"lambda w, {', '.join(argument_names)}: {string}\", function_context, {}) for string in strings]\n",
    "    \n",
    "    # Create the function, which applies softmax over the outputs of the created lambda functions\n",
    "    def func(w, args):\n",
    "        def _softmax(x):\n",
    "            return exp(x)/np.sum(exp(x), axis=0)\n",
    "        return _softmax([f(w, *args) for f in funcs])\n",
    "    \n",
    "    # Return the created function\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add compile function to toolbox\n",
    "toolbox.register(\"compile\", compile_multiclasstree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining the fitness function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define fitness function (cross-entropy loss & inversed number of connections)\n",
    "def fitness(individual, n_samples_to_test, weights_to_test, seed=-1):\n",
    "    # Compile the functions corresponding to the individual\n",
    "    func = toolbox.compile(individual)\n",
    "    \n",
    "    # Define how to compute cross-entropy\n",
    "    def _cross_entropy(pred, label):\n",
    "        return -np.log(exp(pred[label])/np.sum(exp(pred), axis=0))\n",
    "    \n",
    "    # Ensure all individuals in a generation can be tested on the same samples\n",
    "    if seed >=0:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    # Create a list of samples to test, ensuring an equal number of samples from each class\n",
    "    sample_indices = []\n",
    "    n_samples_per_class = int(n_samples_to_test/N_CLASSES_TO_USE)\n",
    "    for c in range(N_CLASSES_TO_USE):\n",
    "        c_indices = np.where(Y_train == c)[0]\n",
    "        assert len(c_indices) >= n_samples_per_class, \\\n",
    "            f\"Class {c} has too few elements to reach the desired number of evaluation samples\"\n",
    "        sample_indices.extend(np.random.permutation(c_indices)[:n_samples_per_class])\n",
    "    \n",
    "    # Compute cross-entropy loss for each of the samples\n",
    "    results = []\n",
    "    for w in weights_to_test:\n",
    "        w_results = []\n",
    "        for i in sample_indices:\n",
    "            X_sample, Y_sample = X_train[i], Y_train[i]\n",
    "            w_results.append(_cross_entropy(func(w, X_sample), Y_sample))\n",
    "        results.append(np.average(w_results))\n",
    "    \n",
    "    # Return the average cross-entropy loss and inverse number of connections\n",
    "    return (np.average(results), np.min(results), 1/individual.n_connections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the fitness function to the toolbox\n",
    "toolbox.register(\"evaluate\", fitness, n_samples_to_test=N_SAMPLES_TO_TEST, weights_to_test=WEIGHTS_TO_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution components\n",
    "\n",
    "##### Parent selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how to select parents (tournament selection based on NSGA2 rank)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=TOURNAMENT_SIZE)#, fit_attr=\"rank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mutating the activation function of a hidden node\n",
    "def _mutate_activation(tree):\n",
    "    # If the tree has no hidden nodes, do nothing\n",
    "    if not tree.hidden:\n",
    "        return\n",
    "        \n",
    "    # Select a random node and give it a different activation function\n",
    "    node = random.choice(tree.hidden)\n",
    "    node.name = random.choice([name for name in function_names if not name == node.name])\n",
    "\n",
    "\n",
    "# Adding a node to the tree\n",
    "def _add_node(tree):\n",
    "    # Select a random parent-child pair between which to place a node\n",
    "    parent = random.choice(tree.outputs + tree.hidden)\n",
    "    child = random.choice(parent.children)\n",
    "        \n",
    "    # Disable the connection between the parent and the child\n",
    "    parent.children.remove(child)\n",
    "    parent.disabled.append(child)\n",
    "\n",
    "    # Create a new node with a random activation function and add it to the tree\n",
    "    new_node = PrimitiveNode(random.choice(function_names))\n",
    "    tree.hidden.append(new_node)\n",
    "        \n",
    "    # Update the parent/child relations\n",
    "    parent.children.append(new_node)\n",
    "    new_node.parents.append(parent)\n",
    "    new_node.children.append(child)\n",
    "    child.parents.append(new_node)\n",
    "        \n",
    "    # Update the number of enabled connections\n",
    "    tree.n_connections += 1\n",
    "\n",
    "    \n",
    "# Adding a connection in the tree\n",
    "def _add_connection(tree):\n",
    "    # Function that checks if node1 is an ancestor of node 2\n",
    "    def _is_ancestor(node1, node2):\n",
    "        return node1 in [node2] + node2.parents or any([_is_ancestor(node1, node3) for node3 in node2.parents])\n",
    "    \n",
    "    # Find a valid parent-child pair for a connection, respecting the feed-forward property (no loops)\n",
    "    valid_connection = False\n",
    "    n_attempts = 0\n",
    "    while not valid_connection and n_attempts < 500:\n",
    "        parent = random.choice(tree.outputs + tree.hidden)\n",
    "        child = random.choice(tree.inputs + tree.hidden)\n",
    "        valid_connection = not _is_ancestor(child, parent) # Connection is valid if child is not an ancestor of parent\n",
    "        n_attempts += 1\n",
    "    \n",
    "    # If a valid connection was found, update the parent/child relations and the number of enabled connections\n",
    "    if n_attempts < 500:\n",
    "        child.parents.append(parent)\n",
    "        parent.children.append(child)\n",
    "        tree.n_connections += 1\n",
    "\n",
    "\n",
    "# Enable a disabled connection (created during initialization or when adding a node)\n",
    "def _enable_connection(tree):\n",
    "    # Check if there are any nodes with disabled connections; if not, do nothing\n",
    "    parents_with_disabled = [node for node in tree.outputs + tree.hidden if node.disabled]\n",
    "    if not parents_with_disabled:\n",
    "        return\n",
    "    \n",
    "    # Select a random disabled parent-child pair\n",
    "    parent = random.choice(parents_with_disabled)\n",
    "    child = random.choice(parent.disabled)\n",
    "    \n",
    "    # Enable the corresponding connection and update the number of enabled connections\n",
    "    parent.disabled.remove(child)\n",
    "    parent.children.append(child)\n",
    "    tree.n_connections += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate(tree):\n",
    "    # Copy the parent tree\n",
    "    tree = toolbox.clone(tree)\n",
    "        \n",
    "    # Create lists of the various mutation functions and the corresponding probabilities\n",
    "    mutation_functions = [_mutate_activation, _add_node, _add_connection, _enable_connection]\n",
    "    probabilities = [P_MUTATE_ACTIVATION, P_ADD_NODE, P_ADD_CONNECTION, P_ENABLE_CONNECTION]\n",
    "    \n",
    "    # Ensure probabilities sum to 1\n",
    "    assert sum(probabilities) == 1, \"Mutation probabilities should sum to 1\"\n",
    "    \n",
    "    # Choose a mutation function using the provided probabilities and execute it\n",
    "    mutation_function, = random.choices(mutation_functions, probabilities, k=1)\n",
    "    mutation_function(tree)\n",
    "    \n",
    "    # Return the resulting tree\n",
    "    return tree,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the mutate function to the toolbox\n",
    "toolbox.register(\"mutate\", mutate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe which kinds of statistics to keep track of\n",
    "stats_avgfit = tools.Statistics(key = lambda ind: ind.fitness.values[0])\n",
    "stats_bestfit = tools.Statistics(key = lambda ind: ind.fitness.values[1])\n",
    "stats_connections = tools.Statistics(key = lambda ind: ind.n_connections)\n",
    "stats_hidden = tools.Statistics(key = lambda ind: len(ind.hidden))\n",
    "\n",
    "# Combine statistics into a single multistatistics object\n",
    "mstats = tools.MultiStatistics(avg_fitness=stats_avgfit, best_fitness=stats_bestfit, \n",
    "                               hidden=stats_hidden, connections=stats_connections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe metrics to keep track of for each statistic\n",
    "mstats.register(\"avg\", np.mean)\n",
    "mstats.register(\"std\", np.std)\n",
    "mstats.register(\"min\", np.min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the genetic programming algorithm\n",
    "\n",
    "##### Defining the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eaWann(population, toolbox, ngen, cull_ratio, elite_ratio, stats=None, halloffame=None, verbose=__debug__):\n",
    "    \"\"\"\n",
    "    Evolutionary algorithm for Weight Agnostic Neural Networks (WANNs)\n",
    "    Based on the algorithms provided by DEAP, as well as the WANN implementation\n",
    "    \n",
    "    The basic idea is as follows:\n",
    "    1. Sort the population based on fitness (using NSGA2)\n",
    "    2. Remove the worst individuals\n",
    "    3. Copy the best individuals directly to the next generation\n",
    "    4. Perform tournament selection to create the remaining offspring\n",
    "    5. Evaluate all individuals in the new population (each individual is tested on the same sample)\n",
    "    6. Repeat from 1\n",
    "    \n",
    "    Parameters:\n",
    "    population: the intial population\n",
    "    toolbox: the DEAP toolbox containing functions for parent selection, mutation etc.\n",
    "    ngen: number of generations to run the algorithm for\n",
    "    cull_ratio: fraction of the population that will be thrown away every generation (worst individuals)\n",
    "    elite_ratio: fraction of the population that will be directly copied to the next generation (best individuals)\n",
    "    stats: (Multi)Statistics object, keeping track of evolution statistics\n",
    "    halloffame: List containing the best individuals that ever lived\n",
    "    verbose: whether or not to print statistics\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize logbook and set the correct headers\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + ['avg_fitness', 'best_fitness', 'connections', 'hidden'] if stats else []\n",
    "    for field in stats.fields:\n",
    "        if \"fitness\" in field:\n",
    "            logbook.chapters[field].header = \"min\", \"avg\", \"std\"\n",
    "        else:\n",
    "            logbook.chapters[field].header = \"avg\", \"std\"\n",
    "\n",
    "    # Evaluate the individuals with invalid fitnesses\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(partial(toolbox.evaluate, seed=0), invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "        ind.born = 0\n",
    "\n",
    "    # Update hall of fame\n",
    "    if halloffame is not None:\n",
    "        halloffame.update(population)\n",
    "        \n",
    "    # Record and print performance if applicable\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1): \n",
    "        \n",
    "        # Initialize offspring and determine offspring size\n",
    "        offspring = []\n",
    "        population_size = len(population)\n",
    "        \n",
    "        # Rank the population and update rank values (tournament selection choses individuals with larger rank)\n",
    "        ranked_population = tools.selNSGA2(population, population_size)\n",
    "        for i in range(population_size):\n",
    "            ranked_population[i].rank = population_size-i\n",
    "        \n",
    "        # Culling - remove worst performing individuals\n",
    "        number_to_cull = int(cull_ratio*population_size)\n",
    "        ranked_population = ranked_population[:population_size-number_to_cull]\n",
    "        \n",
    "        # Elitism - select and copy best performing individuals\n",
    "        number_of_elites = int(elite_ratio*population_size)\n",
    "        for i in range(number_of_elites):\n",
    "            copy = toolbox.clone(ranked_population[i])\n",
    "            del copy.fitness.values # Will be re-evaluated using this generation's sample\n",
    "            offspring.append(copy)\n",
    "            \n",
    "        # Compute number of offspring that still need to be generated\n",
    "        offspring_to_generate = population_size - number_of_elites\n",
    "            \n",
    "        # Select parents via (NSGA2 rank-based) tournament selection\n",
    "        parents = toolbox.select(ranked_population, offspring_to_generate)\n",
    "        \n",
    "        # Mutate parents to obtain children\n",
    "        for parent in parents:\n",
    "            child, = toolbox.mutate(parent)\n",
    "            del child.fitness.values\n",
    "            child.born = gen\n",
    "            offspring.append(child)\n",
    "\n",
    "        # Evaluate all individuals with an invalid fitness (all of them) using the same seed for sampling\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(partial(toolbox.evaluate, seed=gen), invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        if halloffame is not None:\n",
    "            halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \t      \t       avg_fitness       \t       best_fitness      \t  connections  \t   hidden  \n",
      "   \t      \t-------------------------\t-------------------------\t---------------\t-----------\n",
      "gen\tnevals\tmin    \tavg    \tstd      \tmin   \tavg    \tstd      \tavg   \tstd    \tavg\tstd\n",
      "0  \t250   \t2.27947\t2.31348\t0.0133393\t2.2442\t2.29636\t0.0151968\t128.58\t10.5544\t0  \t0  \n",
      "1  \t250   \t2.27364\t2.29153\t0.00870036\t2.22147\t2.27042\t0.0154955\t122.356\t10.595 \t0.204\t0.402969\n",
      "2  \t250   \t2.28124\t2.29064\t0.00382425\t2.24388\t2.27334\t0.0112821\t113.904\t5.04924\t0.408\t0.594589\n",
      "3  \t250   \t2.26424\t2.27933\t0.00618999\t2.22466\t2.24735\t0.0149504\t114.496\t4.69659\t0.888\t0.689533\n",
      "4  \t250   \t2.2648 \t2.27612\t0.00730205\t2.23096\t2.25585\t0.0121296\t118.172\t4.85453\t1.532\t0.912675\n",
      "5  \t250   \t2.26368\t2.27477\t0.00756653\t2.23091\t2.25105\t0.008438 \t118.976\t5.18916\t1.976\t1.09883 \n"
     ]
    }
   ],
   "source": [
    "# Run the evolutionary algorithm\n",
    "pop = toolbox.population(POPULATION_SIZE)\n",
    "hof = tools.HallOfFame(10)\n",
    "pop, log = eaWann(population=pop, toolbox=toolbox, ngen=N_GENERATIONS, cull_ratio=CULL_RATIO, \n",
    "                  elite_ratio=ELITE_RATIO, stats=mstats, halloffame=hof, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Wrapping up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "with open(RESULTS_FILENAME, \"wb\") as f:\n",
    "    pickle.dump((pop, log, hof), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract generation IDs, minimum fitnesses and average total heights per generation\n",
    "gen = log.select(\"gen\")\n",
    "fitness_best = log.chapters[\"avg_fitness\"].select(\"min\") \n",
    "conn_avg = log.chapters[\"connections\"].select(\"avg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot line for minimum fitness\n",
    "fig, fit_ax = plt.subplots()\n",
    "fit_line = fit_ax.plot(gen, fitness_best, \"b-\", label=\"Best Fitness\")\n",
    "fit_ax.set_xlabel(\"Generation\")\n",
    "fit_ax.set_ylabel(f\"Accuracy\", color=\"b\")\n",
    "for tl in fit_ax.get_yticklabels():\n",
    "    tl.set_color(\"b\")\n",
    "\n",
    "# Plot line for average total height\n",
    "height_ax = fit_ax.twinx()\n",
    "height_line = height_ax.plot(gen, conn_avg, \"r-\", label=\"Average Number of Connections\")\n",
    "height_ax.set_ylabel(\"Average height\", color=\"r\")\n",
    "for tl in height_ax.get_yticklabels():\n",
    "    tl.set_color(\"r\")\n",
    "\n",
    "# Add legend\n",
    "lines = fit_line + height_line\n",
    "labs = [l.get_label() for l in lines]\n",
    "fit_ax.legend(lines, labs, loc=\"upper center\")\n",
    "\n",
    "# Show the result\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the best individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ind = hof[0]\n",
    "print(best_ind)\n",
    "print(f\"Fitness of best individual: {best_ind.fitness}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Printing trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the trees of the best individual\n",
    "for string in best_ind.get_strings():\n",
    "    print(f\"{string}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Computing training & validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving predictions from an individual\n",
    "def get_predictions(individual, X, weight):\n",
    "    func = compile_multiclasstree(individual)\n",
    "    predictions = []\n",
    "    for i in range(X.shape[0]):\n",
    "        outputs_i = func(weight, X[i])\n",
    "        predictions.append(np.argmax(outputs_i))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy of predictions\n",
    "def compute_accuracy(Y_pred, Y_true):\n",
    "    n_correct = np.sum(Y_pred == Y_true)\n",
    "    return n_correct/Y_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve predictions of the best individual on the training and validation sets, for all weights\n",
    "Y_train_pred = np.array([get_predictions(best_ind, X_train, w) for w in WEIGHTS_TO_TEST])\n",
    "Y_test_pred = np.array([get_predictions(best_ind, X_test, w) for w in WEIGHTS_TO_TEST])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute training accuracies for all weights and extract the best weight\n",
    "train_accs = [compute_accuracy(Y_train_pred[i], Y_train) for i in range(len(WEIGHTS_TO_TEST))]\n",
    "test_accs = [compute_accuracy(Y_test_pred[i], Y_test) for i in range(len(WEIGHTS_TO_TEST))]\n",
    "best_weight_idx_train = np.argmax(train_accs)\n",
    "best_weight_idx_test = np.argmax(test_accs)\n",
    "\n",
    "# Print best training and validation accuracies of the best individual\n",
    "print(f\"Best training accuracy (weight {WEIGHTS_TO_TEST[best_weight_idx_train]}): {np.max(train_accs)}\")\n",
    "print(f\"Best validation accuracy (weight {WEIGHTS_TO_TEST[best_weight_idx_test]}): {np.max(test_accs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain majority votes (in case of a tie, use vote of the best classifier)\n",
    "def get_majority_predictions(predictions):\n",
    "    def _majority(l):\n",
    "        return max(set(l), key=l.count)\n",
    "    predictions = [_majority(list(predictions[:,i])) for i in range(predictions.shape[1])]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute majority predictions for train and validation sets\n",
    "Y_train_majpred = get_majority_predictions(Y_train_pred)\n",
    "Y_test_majpred = get_majority_predictions(Y_test_pred)\n",
    "\n",
    "# Print the accuracies of the majority votes\n",
    "print(f\"Majority training accuracy: {compute_accuracy(Y_train_majpred, Y_train)}\")\n",
    "print(f\"Majority validation accuracy: {compute_accuracy(Y_test_majpred, Y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrices\n",
    "cm_train_best = confusion_matrix(Y_train, Y_train_pred[best_weight_idx_train], labels=range(N_CLASSES_TO_USE))\n",
    "cm_test_best = confusion_matrix(Y_test, Y_test_pred[best_weight_idx_test], labels=range(N_CLASSES_TO_USE))\n",
    "cm_train_maj = confusion_matrix(Y_train, Y_train_majpred, labels=range(N_CLASSES_TO_USE))\n",
    "cm_test_maj = confusion_matrix(Y_test, Y_test_majpred, labels=range(N_CLASSES_TO_USE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "fig, ax = plt.subplots(2,2, figsize=(11,10))\n",
    "ax = ax.ravel()\n",
    "\n",
    "# Plot confusion matrix for training data using best weight\n",
    "sns.heatmap(cm_train_best, annot=True, fmt='g', ax=ax[0], cmap=\"Blues\")\n",
    "ax[0].set_xlabel('Predicted labels')\n",
    "ax[0].set_ylabel('True labels')\n",
    "ax[0].set_title('Confusion matrix for training data (best weight)')\n",
    "\n",
    "# Plot confusion matrix for validation data using best weight\n",
    "sns.heatmap(cm_test_best, annot=True, fmt='g', ax=ax[1], cmap=\"Blues\")\n",
    "ax[1].set_xlabel('Predicted labels')\n",
    "ax[1].set_ylabel('True labels')\n",
    "ax[1].set_title('Confusion matrix for validation data (best weight)')\n",
    "\n",
    "# Plot confusion matrix for training data using majority vote\n",
    "sns.heatmap(cm_train_maj, annot=True, fmt='g', ax=ax[2], cmap=\"Blues\")\n",
    "ax[2].set_xlabel('Predicted labels')\n",
    "ax[2].set_ylabel('True labels')\n",
    "ax[2].set_title('Confusion matrix for training data (majority vote)')\n",
    "\n",
    "# Plot confusion matrix for validation data using majority vote\n",
    "sns.heatmap(cm_test_maj, annot=True, fmt='g', ax=ax[3], cmap=\"Blues\")\n",
    "ax[3].set_xlabel('Predicted labels')\n",
    "ax[3].set_ylabel('True labels')\n",
    "ax[3].set_title('Confusion matrix for validation data (majority vote)')\n",
    "\n",
    "# Show the result\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Used features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract input tallies from best individual\n",
    "input_tallies = []\n",
    "for tree in best_ind.get_strings():\n",
    "    inputs_used = list(map(int, re.findall(\"[0-9]+\", str(tree))))\n",
    "    input_tally = np.zeros(X_train.shape[1])\n",
    "    for arg in inputs_used:\n",
    "        input_tally[arg] += 1\n",
    "    input_tallies.append(input_tally)\n",
    "\n",
    "# Create plots of the inputs (pixels) used in the tree of the best individual for each class\n",
    "# Note: for the best-looking plot, this implementation assumes that N_CLASSES_TO_USE is set to 10\n",
    "fig, ax = plt.subplots(2, 5, figsize=(16,5))\n",
    "ax = ax.ravel()\n",
    "for i, tally in enumerate(input_tallies):\n",
    "    img_shape = int(math.sqrt(X_train.shape[1]))\n",
    "    ax[i].imshow(np.array(tally).reshape(img_shape, img_shape), clim=(0,np.max(input_tallies)))\n",
    "    ax[i].axis(\"off\")\n",
    "    ax[i].set_title(f\"Pixels used for class {i}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.sum(input_tallies, axis=0).reshape(16,16))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Pixels used in the entire tree\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
